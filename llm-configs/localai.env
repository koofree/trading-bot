# LocalAI Configuration
# OpenAI-compatible local API

LLM_PROVIDER=localai
LLM_MODEL=ggml-gpt4all-j
LLM_BASE_URL=http://localhost:8080/v1
# LLM_API_KEY=optional_api_key

# LocalAI supports various models:
# - ggml-gpt4all-j
# - ggml-koala
# - ggml-vicuna
# - ggml-alpaca
# - gpt-3.5-turbo (local implementation)

# To start LocalAI:
# docker run -p 8080:8080 -v $PWD/models:/models -ti --rm quay.io/go-skynet/local-ai:latest