# Ollama Configuration
# Local LLM using Ollama

LLM_PROVIDER=ollama
LLM_MODEL=llama2
LLM_BASE_URL=http://localhost:11434

# Available Ollama models (install with: ollama pull <model>)
# - llama2 (7B parameters)
# - mistral (7B parameters, faster)
# - codellama (code-optimized)
# - neural-chat (Intel's fine-tuned model)
# - starling-lm (Berkeley's model)
# - dolphin-mixtral (uncensored)
# - openhermes (fine-tuned on diverse data)

# To use a different model, change LLM_MODEL above
# Example: LLM_MODEL=mistral