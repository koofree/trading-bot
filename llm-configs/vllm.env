# vLLM Configuration
# High-performance inference server with PagedAttention

LLM_PROVIDER=vllm
LLM_MODEL=meta-llama/Llama-2-7b-chat-hf
LLM_BASE_URL=http://localhost:8000

# vLLM supports HuggingFace models directly
# Popular models for vLLM:
# - meta-llama/Llama-2-7b-chat-hf (Llama 2 7B)
# - meta-llama/Llama-2-13b-chat-hf (Llama 2 13B)
# - mistralai/Mistral-7B-Instruct-v0.1 (Mistral 7B)
# - NousResearch/Meta-Llama-3-8B-Instruct (Llama 3 8B)
# - codellama/CodeLlama-7b-Instruct-hf (CodeLlama 7B)

# To start vLLM server:
# python -m vllm.entrypoints.openai.api_server \
#   --model meta-llama/Llama-2-7b-chat-hf \
#   --port 8000 \
#   --max-model-len 4096